# üé≠ RAG Events Assistant

> **Assistant conversationnel intelligent** pour d√©couvrir des √©v√©nements culturels via des questions en langage naturel

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-LCEL-blueviolet.svg)](https://python.langchain.com/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.41+-red.svg)](https://streamlit.io/)
[![Mistral AI](https://img.shields.io/badge/Mistral%20AI-latest-orange.svg)](https://mistral.ai/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üìñ Table des Mati√®res

- [üéØ √Ä Propos du Projet](#-√†-propos-du-projet)
- [‚ú® Fonctionnalit√©s](#-fonctionnalit√©s)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [üöÄ Installation](#-installation)
- [üìä Utilisation](#-utilisation)
- [üîå API Reference](#-api-reference)
- [üß™ Tests & √âvaluation](#-tests--√©valuation)
- [üê≥ Docker](#-docker)
- [üìö Documentation](#-documentation)
- [ü§ù Contribution](#-contribution)
- [üìÑ License](#-license)

---

## üéØ √Ä Propos du Projet

**RAG Events Assistant** est un syst√®me de **Retrieval-Augmented Generation (RAG)** qui combine la puissance de la recherche s√©mantique avec l'intelligence des LLM pour aider les utilisateurs √† d√©couvrir des √©v√©nements culturels pertinents.

### Qu'est-ce que le RAG ?

Le RAG (Retrieval-Augmented Generation) est une architecture d'IA qui enrichit les r√©ponses des LLM avec des informations r√©cup√©r√©es depuis une base de connaissances externe :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      PIPELINE RAG                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Question utilisateur                                            ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  Classification intelligente                                     ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   CHAT Mode        ‚îÇ   SEARCH Mode      ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   (Simple LLM)     ‚îÇ   (RAG Pipeline)   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   ‚Üì                ‚îÇ   ‚Üì                ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   Conversation     ‚îÇ   1. Embedding     ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   directe          ‚îÇ   2. FAISS Search  ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ   3. LLM + Context ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  R√©ponse contextuelle + Sources                                 ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> **Architecture interne** : Chaque √©tape utilise des **cha√Ænes LCEL** (LangChain Expression Language) composables via l'op√©rateur `|` : `prompt | llm | parser`

### Pourquoi ce projet ?

- **üéì P√©dagogique** : Impl√©mentation compl√®te d'un syst√®me RAG moderne
- **üèóÔ∏è Production-ready** : API REST, Docker, tests, √©valuation RAGAS
- **üá´üá∑ Multilingue** : Support du fran√ßais avec embeddings Mistral
- **‚ö° Performant** : FAISS pour la recherche vectorielle ultra-rapide
- **üîß Flexible** : Support de plusieurs providers d'embeddings

### Stack Technologique

| Composant | Technologie | R√¥le |
|-----------|-------------|------|
| **Orchestration** | LangChain LCEL | Composition de cha√Ænes RAG modulaires |
| **LLM** | Mistral AI via `langchain-mistralai` | G√©n√©ration de r√©ponses conversationnelles |
| **Embeddings** | Mistral Embed / HuggingFace via LangChain | Vectorisation s√©mantique multilingue |
| **Vector Store** | FAISS via `langchain-community` | Recherche de similarit√© ultra-rapide |
| **API** | FastAPI | REST API avec sessions et background tasks |
| **UI** | Streamlit | Interface chat moderne et r√©active |
| **Data** | Open Agenda API | Source d'√©v√©nements culturels |
| **Tests** | pytest + RAGAS | Tests unitaires/int√©gration + √©valuation RAG |
| **Package Manager** | uv | Gestionnaire de d√©pendances moderne |

---

## ‚ú® Fonctionnalit√©s

### ü§ñ Intelligence Conversationnelle

- **Classification automatique** : Distingue questions conversationnelles vs recherches d'√©v√©nements
- **M√©moire contextuelle** : Garde en m√©moire les 5 derniers √©changes
- **R√©ponses naturelles** : G√©n√©ration en fran√ßais avec ton conversationnel

### üîç Recherche S√©mantique

- **Embeddings multilingues** : Support Mistral Embed (1024d) et Sentence Transformers (768d)
- **Recherche FAISS** : Top-k retrieval avec scores de similarit√©
- **M√©tadonn√©es riches** : Date, lieu, prix, URL, description

### üåê API REST & Interface

- **FastAPI** : Endpoints `/search`, `/chat`, `/session`, `/rebuild`
- **Streamlit** : Interface chat moderne avec th√®me sombre
- **Sessions** : Gestion de conversations multi-utilisateurs
- **Background tasks** : Rebuild d'index sans bloquer l'API

### üìä √âvaluation & Monitoring

- **RAGAS Integration** : M√©triques de context precision, faithfulness, relevance
- **M√©triques custom** : Keyword coverage, latency, success rate
- **Rapports JSON** : R√©sultats d'√©valuation d√©taill√©s

---

## üèóÔ∏è Architecture

### Structure du Projet

```
.
‚îú‚îÄ‚îÄ üì± app.py                      # Interface Streamlit
‚îú‚îÄ‚îÄ üì¶ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/                   # Configuration & constantes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py           # Pydantic Settings (env vars)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constants.py          # Constantes (prompts, paths, thresholds)
‚îÇ   ‚îú‚îÄ‚îÄ data/                     # Mod√®les de donn√©es
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py             # Pydantic models (Event, QueryResponse, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ rag/                      # Moteur RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine.py             # RAGEngine (classification, search, generation)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index_builder.py     # IndexBuilder (FAISS index construction)
‚îÇ   ‚îî‚îÄ‚îÄ api/                      # API REST
‚îÇ       ‚îî‚îÄ‚îÄ main.py               # FastAPI app (endpoints + sessions)
‚îú‚îÄ‚îÄ üìì notebooks/                  # Pipeline de donn√©es (01-05)
‚îú‚îÄ‚îÄ üß™ tests/                      # Tests unitaires/int√©gration/e2e
‚îú‚îÄ‚îÄ üìö docs/                       # Documentation compl√®te
‚îú‚îÄ‚îÄ üê≥ Dockerfile                  # Multi-stage build
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml          # Orchestration API + Streamlit
‚îî‚îÄ‚îÄ ‚öôÔ∏è pyproject.toml              # Configuration projet (uv)
```

### Modules Principaux

#### **RAGEngine** (`src/rag/engine.py`)

Le c≈ìur du syst√®me RAG orchestr√© par **3 cha√Ænes LCEL** :

- **Classification Chain** : `needs_rag(query)` ‚Üí Routage CHAT vs SEARCH
- **Conversation Chain** : `conversation_response(query, history)` ‚Üí Mode CHAT (sans contexte)
- **RAG Chain** : `generate_response(query, context, history)` ‚Üí Mode SEARCH (avec contexte)
- `search(query, top_k)` : Recherche s√©mantique via `FAISS.similarity_search_with_score()`
- `chat(query, history)` : Pipeline complet unifi√© avec d√©tection automatique

#### **Composants LangChain** (`src/rag/`)

| Module | Fonction | Composant LangChain |
|--------|----------|---------------------|
| `embeddings.py` | `get_embeddings()` | `MistralAIEmbeddings` / `HuggingFaceEmbeddings` |
| `llm.py` | `get_llm()` | `ChatMistralAI` avec param√®tres configurables |
| `vectorstore.py` | `load/build/save_vectorstore()` | `FAISS` de `langchain-community` |

#### **IndexBuilder** (`src/rag/index_builder.py`)

Construction et gestion des index FAISS via LangChain :

- `load_documents()` : Chargement des √©v√©nements vers `Document` LangChain
- `build_and_save()` : Construction via `FAISS.from_documents()` avec batch processing
- `rebuild()` : Pipeline complet avec callbacks de progression

### Cha√Ænes LCEL

Le syst√®me utilise **LangChain Expression Language (LCEL)** pour composer des pipelines modulaires et testables :

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

# Classification Chain - D√©termine SEARCH vs CHAT
classification_chain = ChatPromptTemplate | LLM(temperature=0) | StrOutputParser

# Conversation Chain - Mode CHAT (sans contexte RAG)
conversation_chain = ChatPromptTemplate([
    ("system", SYSTEM_PROMPT),
    MessagesPlaceholder("history"),
    ("human", "{query}")
]) | LLM | StrOutputParser

# RAG Chain - Mode SEARCH (avec contexte inject√©)
rag_chain = ChatPromptTemplate([
    ("system", RAG_PROMPT_WITH_CONTEXT),
    MessagesPlaceholder("history"),
    ("human", "{query}")
]) | LLM | StrOutputParser
```

> üìö Voir **[INTEGRATION_LANGCHAIN.md](docs/INTEGRATION_LANGCHAIN.md)** pour l'architecture compl√®te et **[GUIDE_LANGCHAIN.md](docs/GUIDE_LANGCHAIN.md)** pour un guide p√©dagogique.

#### **FastAPI** (`src/api/main.py`)

API REST compl√®te avec :

- **Sessions** : Stockage en m√©moire avec historique (max 5 messages)
- **Endpoints** :
  - `GET /health` : Health check
  - `POST /search` : Recherche sans session
  - `POST /chat` : Chat avec session auto-cr√©√©e
  - `GET/DELETE /session/{id}` : Gestion de sessions
  - `POST /rebuild` : Rebuild background avec auth API key
- **CORS** : Configuration pour int√©gration frontend
- **Background Tasks** : Rebuild non-bloquant

---

## üöÄ Installation

### Pr√©requis

- **Python 3.11+** ([Download](https://www.python.org/downloads/))
- **uv** ([Installation](https://github.com/astral-sh/uv))
  ```bash
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```
- **Cl√© API Mistral AI** ([Console Mistral](https://console.mistral.ai/))

### Installation Locale

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/OC7---Projet-RAG-Assistant-Intelligent-Events.git
cd OC7---Projet-RAG-Assistant-Intelligent-Events

# 2. Installer les d√©pendances
uv sync --all-extras

# 3. Configurer les variables d'environnement
cp .env.example .env
# √âditer .env et ajouter votre MISTRAL_API_KEY

# 4. Pr√©parer les donn√©es (voir section suivante)
uv run jupyter lab  # Ex√©cuter notebooks 01-04
```

---

## üìä Utilisation

### 1Ô∏è‚É£ Pr√©paration des Donn√©es

Les notebooks Jupyter pr√©parent les donn√©es dans l'ordre :

```bash
uv run jupyter lab
```

**Pipeline complet** :

1. **`01_data_collection.ipynb`** : R√©cup√©ration depuis Open Agenda API
2. **`02_data_preprocessing.ipynb`** : Nettoyage HTML et structuration
3. **`03_create_embeddings_mistral.ipynb`** : G√©n√©ration embeddings (Mistral ou ST)
4. **`04_build_faiss_index.ipynb`** : Construction index FAISS
5. **`05_rag_chatbot_mistral.ipynb`** : Test et validation du syst√®me

üìö Voir [notebooks/README.md](notebooks/README.md) pour plus de d√©tails

### 2Ô∏è‚É£ Lancer l'Application

#### Option A : Interface Streamlit

```bash
uv run streamlit run app.py
```

Ouvrir http://localhost:8501 dans votre navigateur.

**Fonctionnalit√©s UI** :
- Chat conversationnel avec m√©moire
- Th√®me sombre moderne
- Affichage des sources et scores
- Param√®tres ajustables (top_k, temp√©rature)

#### Option B : API REST

```bash
uv run uvicorn src.api.main:app --reload
```

- **API** : http://localhost:8000
- **Documentation Swagger** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc

---

## üîå API Reference

### Endpoints Principaux

#### üè• Health Check

```bash
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "index_loaded": true,
  "num_documents": 497
}
```

#### üîç Recherche S√©mantique

```bash
POST /search
Content-Type: application/json

{
  "query": "concerts jazz √† Paris ce weekend",
  "top_k": 5
}
```

**Response:**
```json
{
  "query": "concerts jazz √† Paris ce weekend",
  "response": "Voici 3 concerts de jazz √† Paris ce weekend...",
  "sources": [
    {
      "title": "Paris Jazz Festival",
      "city": "Paris",
      "start_date": "2025-01-18T20:00:00",
      "url": "https://...",
      "score": 0.87
    }
  ],
  "processing_time": 1.23
}
```

#### üí¨ Chat avec Session

```bash
POST /chat
Content-Type: application/json

{
  "query": "Bonjour, peux-tu me recommander un concert ?",
  "session_id": "optional-custom-id"
}
```

**Response:**
```json
{
  "session_id": "abc-123-def",
  "query": "Bonjour, peux-tu me recommander un concert ?",
  "response": "Bonjour ! Bien s√ªr, voici quelques concerts...",
  "sources": [...],
  "processing_time": 1.45
}
```

**Continuer la conversation:**

```bash
POST /chat
Content-Type: application/json

{
  "query": "Et √† Marseille ?",
  "session_id": "abc-123-def"
}
```

#### üîÑ Rebuild Index

```bash
POST /rebuild
Content-Type: application/json
X-API-Key: your-rebuild-api-key

{
  "events": [...],
  "use_mistral_embeddings": true
}
```

**Response:**
```json
{
  "status": "started",
  "task_id": "rebuild-task-123"
}
```

### Gestion de Sessions

```bash
# R√©cup√©rer historique
GET /session/{session_id}

# Supprimer session
DELETE /session/{session_id}
```

---

## üß™ Tests & √âvaluation

### Tests Unitaires & Int√©gration

```bash
# Tous les tests
uv run pytest

# Avec couverture
uv run pytest --cov=src --cov-report=html
# Voir htmlcov/index.html

# Tests sp√©cifiques
uv run pytest tests/unit/              # Tests unitaires
uv run pytest tests/integration/       # Tests d'int√©gration
uv run pytest tests/e2e/               # Tests end-to-end

# Exclure tests lents
uv run pytest -m "not slow"
```

### √âvaluation RAGAS

```bash
# √âvaluer le syst√®me RAG
uv run python scripts/evaluate_rag.py --test-file tests/data/test_questions.json

# Voir les r√©sultats
cat tests/data/evaluation_results.json
```

**M√©triques √©valu√©es :**

| M√©trique | Description | Cible |
|----------|-------------|-------|
| **Latency** | Temps de r√©ponse moyen | < 3.0s |
| **Relevance** | Coverage des mots-cl√©s attendus | > 80% |
| **Success Rate** | Taux de r√©ponses r√©ussies | > 70% |
| **RAGAS Scores** | Context precision, faithfulness | > 0.7 |

### Qualit√© du Code

```bash
# Formatage automatique
uv run black src tests scripts

# Linting
uv run ruff check src tests scripts

# Type checking
uv run mypy src
```

---

## üê≥ Docker

### Build & Run

```bash
# Build l'image
docker build -t rag-events-assistant .

# Run Streamlit (par d√©faut)
docker run -p 8501:8501 --env-file .env \
  -v $(pwd)/data:/app/data \
  rag-events-assistant

# Run FastAPI
docker run -p 8000:8000 --env-file .env \
  -v $(pwd)/data:/app/data \
  rag-events-assistant api
```

### Docker Compose

```bash
# Lancer API + Streamlit
docker-compose up

# Lancer uniquement l'API
docker-compose up api

# Lancer uniquement Streamlit
docker-compose up streamlit

# Logs en temps r√©el
docker-compose logs -f

# Arr√™ter
docker-compose down
```

**Services disponibles :**
- **API** : http://localhost:8000
- **Streamlit** : http://localhost:8501

---

## üìö Documentation

Documentation compl√®te dans le dossier `docs/` :

| Document | Description |
|----------|-------------|
| **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** | Architecture syst√®me d√©taill√©e |
| **[INTEGRATION_LANGCHAIN.md](docs/INTEGRATION_LANGCHAIN.md)** | Architecture LangChain LCEL compl√®te |
| **[GUIDE_LANGCHAIN.md](docs/GUIDE_LANGCHAIN.md)** | Guide p√©dagogique LangChain (concepts, patterns) |
| **[COMPRENDRE_LE_RAG.md](docs/COMPRENDRE_LE_RAG.md)** | Guide p√©dagogique sur le RAG |
| **[GUIDE_DEMARRAGE.md](docs/GUIDE_DEMARRAGE.md)** | Guide de d√©marrage complet |
| **[REFERENCE_API.md](docs/REFERENCE_API.md)** | Documentation API compl√®te |
| **[CLAUDE.md](CLAUDE.md)** | Instructions pour Claude Code |

---

## ‚öôÔ∏è Configuration

Variables d'environnement (`.env`) :

| Variable | Description | D√©faut | Requis |
|----------|-------------|--------|--------|
| `MISTRAL_API_KEY` | Cl√© API Mistral AI | - | ‚úÖ |
| `REBUILD_API_KEY` | Cl√© pour endpoint `/rebuild` | - | ‚ùå |
| `EMBEDDING_PROVIDER` | `mistral` ou `sentence-transformers` | `mistral` | ‚ùå |
| `MISTRAL_EMBEDDING_MODEL` | Mod√®le d'embeddings Mistral | `mistral-embed` | ‚ùå |
| `SENTENCE_TRANSFORMER_MODEL` | Mod√®le ST alternatif | `paraphrase-multilingual-mpnet-base-v2` | ‚ùå |
| `LLM_MODEL` | Mod√®le LLM Mistral | `mistral-small-latest` | ‚ùå |
| `LLM_TEMPERATURE` | Temp√©rature g√©n√©ration (0-2) | `0.7` | ‚ùå |
| `TOP_K_RESULTS` | Nombre de r√©sultats FAISS | `5` | ‚ùå |
| `MIN_SIMILARITY_SCORE` | Seuil de similarit√© | `0.3` | ‚ùå |
| `DEFAULT_LOCATION` | Ville par d√©faut | `marseille` | ‚ùå |

---

## üîß D√©veloppement

### Structure de D√©veloppement

```bash
# Installer avec d√©pendances dev
uv sync --all-extras

# Pre-commit hooks
uv run pre-commit install
uv run pre-commit run --all-files
```

### Workflow de Contribution

1. **Fork** le repository
2. **Cr√©er une branche** : `git checkout -b feature/ma-feature`
3. **Impl√©menter** avec tests
4. **Formatter** : `uv run black . && uv run ruff check .`
5. **Tests** : `uv run pytest --cov=src`
6. **Commit** : `git commit -m "feat: Description"`
7. **Push** : `git push origin feature/ma-feature`
8. **Pull Request** avec description d√©taill√©e

### Conventions de Code

- **Formatage** : Black (line length 100)
- **Linting** : Ruff
- **Type hints** : Python 3.11+ syntax (`list[str]` not `List[str]`)
- **Docstrings** : Google style
- **Commits** : [Conventional Commits](https://www.conventionalcommits.org/)

---

## üìà Roadmap

### ‚úÖ Impl√©ment√©

- [x] Pipeline RAG complet (RAGEngine, IndexBuilder)
- [x] API REST FastAPI avec sessions
- [x] Interface Streamlit moderne
- [x] Support multi-providers (Mistral + SentenceTransformers)
- [x] Tests unitaires/int√©gration/e2e
- [x] √âvaluation RAGAS
- [x] Docker + docker-compose
- [x] Documentation compl√®te

### üöß En Cours / Futur

- [ ] Persistance sessions (Redis/PostgreSQL)
- [ ] Authentification utilisateurs
- [ ] M√©moire de conversation avanc√©e (vector memory)
- [ ] Support multi-langues (EN, ES)
- [ ] Cache intelligent pour embeddings
- [ ] Monitoring Prometheus + Grafana
- [ ] CI/CD GitHub Actions
- [ ] Deployment Kubernetes

---

## ü§ù Contribution

Les contributions sont les bienvenues ! Voir [CONTRIBUTING.md](CONTRIBUTING.md) pour plus de d√©tails.

**Types de contributions :**
- üêõ Bug fixes
- ‚ú® Nouvelles fonctionnalit√©s
- üìù Am√©liorations de documentation
- üß™ Tests suppl√©mentaires
- üåç Traductions

---

## üìÑ License

Ce projet est sous licence **MIT**. Voir [LICENSE](LICENSE) pour plus de d√©tails.

---

## üôè Remerciements

- **[LangChain](https://python.langchain.com/)** : Framework d'orchestration LLM avec LCEL
- **[Mistral AI](https://mistral.ai/)** : LLM et embeddings fran√ßais de qualit√©
- **[FAISS](https://github.com/facebookresearch/faiss)** : Biblioth√®que de recherche vectorielle ultra-rapide
- **[Open Agenda](https://openagenda.com/)** : API d'√©v√©nements culturels
- **[Streamlit](https://streamlit.io/)** : Framework UI r√©actif
- **[FastAPI](https://fastapi.tiangolo.com/)** : Framework API moderne
- **[uv](https://github.com/astral-sh/uv)** : Gestionnaire de paquets Python rapide

---

## üìû Contact & Support

- **Issues** : [GitHub Issues](https://github.com/votre-username/OC7---Projet-RAG-Assistant-Intelligent-Events/issues)
- **Discussions** : [GitHub Discussions](https://github.com/votre-username/OC7---Projet-RAG-Assistant-Intelligent-Events/discussions)

---

<div align="center">

**Fait avec ‚ù§Ô∏è et ‚òï en France**

[‚¨Ü Retour en haut](#-rag-events-assistant)

</div>
