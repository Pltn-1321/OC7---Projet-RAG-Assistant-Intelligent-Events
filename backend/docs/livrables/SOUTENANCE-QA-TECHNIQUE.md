# Soutenance - Questions/R√©ponses Techniques
## RAG Chatbot √âv√©nements Culturels

> **Sheetcode ultra-condens√© pour pr√©paration orale**
> Focus : Backend, RAG, FAISS, Embeddings, Architecture

---

## üìê SECTION 1 : ARCHITECTURE RAG & PIPELINE

### **Q1 : Qu'est-ce que le RAG et pourquoi avoir choisi cette architecture ?**

‚Üí **RAG** (Retrieval-Augmented Generation) = technique qui combine recherche s√©mantique (retrieval) + g√©n√©ration LLM. Au lieu de fine-tuner un mod√®le (co√ªteux, statique), on injecte du **contexte externe** (√©v√©nements) dans le prompt du LLM √† chaque requ√™te.
‚Üí **Avantages** : Donn√©es actualisables en temps r√©el, tra√ßabilit√© des sources (pas d'hallucinations), co√ªt r√©duit vs fine-tuning. Id√©al pour √©v√©nements culturels changeants (Open Agenda).

---

### **Q2 : Comment fonctionne votre pipeline dual-mode (CHAT vs SEARCH) ?**

‚Üí **Classification intelligente** : Chaque requ√™te passe d'abord par `needs_rag()` qui utilise un LLM (temp√©rature 0 = d√©terministe) pour d√©tecter si c'est une recherche d'√©v√©nements (**SEARCH**) ou conversation g√©n√©rale (**CHAT**).
‚Üí **CHAT** : R√©ponse directe LLM sans recherche FAISS (ex: "Bonjour", "Merci").
‚Üí **SEARCH** : Pipeline RAG complet ‚Üí encode query ‚Üí FAISS search (top-k=5) ‚Üí inject context ‚Üí LLM generate.
‚Üí **Performance** : √âconomise 50-70% appels FAISS pour queries non-RAG, maintient UX conversationnelle naturelle.

---

### **Q3 : Pourquoi utiliser LangChain LCEL plut√¥t qu'une impl√©mentation custom ?**

‚Üí **LCEL** (LangChain Expression Language) = syntaxe d√©clarative pour composer des cha√Ænes : `chain = prompt | llm | parser`.
‚Üí **Avantages** : Portabilit√© (changement provider facile), abstractions standardis√©es (MessagesPlaceholder pour historique), support natif vectorstores (FAISS, Pinecone).
‚Üí **Inconv√©nient** : Overhead performance ~10% vs custom, d√©pendance externe.
‚Üí **3 cha√Ænes LCEL** : `_classification_chain`, `_conversation_chain`, `_rag_chain` (voir `engine.py:96-126`).

---

### **Q4 : D√©crivez le flux complet d'une requ√™te /chat**

‚Üí **√âtapes** :
1. **FastAPI** re√ßoit POST /chat (query, session_id, top_k)
2. **Session management** : R√©cup√®re ou cr√©e session PostgreSQL, charge historique (MAX_HISTORY=5 derniers messages)
3. **RAGEngine.chat()** :
   - Classification `needs_rag()` ‚Üí SEARCH ou CHAT
   - Si SEARCH : encode query (Mistral-embed) ‚Üí FAISS search (top-k=5) ‚Üí format context ‚Üí RAG chain
   - Si CHAT : conversation chain direct (pas de FAISS)
4. **Response** : G√©n√©ration LLM (temp√©rature 0.7) + sources (si RAG)
5. **Persistence** : Sauvegarde user + assistant messages en DB avec latency_ms, sources JSON

---

### **Q5 : Comment fonctionne la classification de requ√™tes (needs_rag) ?**

‚Üí **Prompt CLASSIFICATION_PROMPT_TEMPLATE** : "Analyse cette requ√™te et r√©ponds uniquement par 'SEARCH' ou 'CHAT'". Exemples fournis ("Bonjour" ‚Üí CHAT, "Concerts Paris" ‚Üí SEARCH).
‚Üí **LLM config** : `ChatMistralAI(temperature=0, max_tokens=10)` = d√©terministe, r√©ponse binaire.
‚Üí **Cha√Æne LCEL** : `prompt | classification_llm | StrOutputParser()` ‚Üí d√©tecte "SEARCH" dans output.
‚Üí **Accuracy** : 100% sur dataset test (12/12 questions correctement class√©es).

---

### **Q6 : Comment g√©rez-vous l'historique conversationnel ?**

‚Üí **Stockage** : PostgreSQL avec mod√®le `MessageModel` (session_id, role, content, created_at, sources JSON).
‚Üí **Limite** : `MAX_HISTORY = 5` paires user+assistant (10 messages max) pour √©viter d√©passement tokens.
‚Üí **Conversion** : Fonction `_convert_history()` transforme `{"role": "user", "content": "..."}` ‚Üí `HumanMessage` / `AIMessage` (format LangChain).
‚Üí **Injection LCEL** : `MessagesPlaceholder(variable_name="history")` dans prompts conversation/RAG injecte historique dans contexte LLM.

---

## üîç SECTION 2 : FAISS & EMBEDDINGS

### **Q7 : Qu'est-ce que FAISS et pourquoi l'utiliser vs Pinecone/ChromaDB ?**

‚Üí **FAISS** (Facebook AI Similarity Search) = biblioth√®que C++ optimis√©e pour recherche vectorielle haute performance. IndexFlatL2 = recherche exhaustive avec distance L2 euclidienne.
‚Üí **Avantages** : Gratuit, local (pas de cloud), ultra-rapide (50-100ms pour 497 docs), contr√¥le total donn√©es.
‚Üí **vs Pinecone** : Cloud, payant, scalable (millions vecteurs) mais vendor lock-in.
‚Üí **vs ChromaDB** : Plus simple mais moins performant (~2x plus lent).
‚Üí **Choix** : Pour 497 √©v√©nements, FAISS in-memory largement suffisant.

---

### **Q8 : Quelle est la structure de votre index FAISS ?**

‚Üí **Type** : `IndexFlatL2` = exhaustive search (pas de quantization), garantit pr√©cision maximale.
‚Üí **Format LangChain** : 3 fichiers cr√©√©s par `save_local()` :
   - `index.faiss` : Index binaire FAISS
   - `index.pkl` : Docstore + mapping (pickle serialization)
   - `config.json` : M√©tadonn√©es custom (nombre docs, timestamp, provider)
‚Üí **Dimension** : 1024d (mistral-embed)
‚Üí **Normalisation** : Vecteurs normalis√©s L2 (`faiss.normalize_L2()`) pour conversion distance ‚Üí similarit√© cosinus.

---

### **Q9 : Pourquoi mistral-embed 1024d plut√¥t que sentence-transformers 768d ?**

‚Üí **mistral-embed** : Mod√®le Mistral AI sp√©cialis√© fran√ßais, 1024 dimensions, **s√©mantique riche** (contexte culturel fran√ßais natif).
‚Üí **vs sentence-transformers** : 768d, multilingue g√©n√©raliste mais moins performant sur nuances fran√ßaises.
‚Üí **Trade-off** : 1024d = +33% m√©moire vs 768d, mais qualit√© s√©mantique sup√©rieure pour √©v√©nements culturels fran√ßais.
‚Üí **API unifi√©e** : Mistral embed + LLM m√™me provider (simplification gestion cl√©s API).

---

### **Q10 : Comment calculez-vous le score de similarit√© affich√© dans le chat ?**

‚Üí **FAISS retourne distance L2** (plus petit = plus proche). Formule conversion : `similarity = 1 - distance_L2`.
‚Üí **Exemple** : distance=0.2 ‚Üí similarity=0.8 = **80%** affich√©.
‚Üí **Normalisation L2** : Pour vecteurs normalis√©s, distance L2 ‚àà [0, 2]. En pratique r√©sultats pertinents < 1 donc similarity > 0%.
‚Üí **Code** (`engine.py:232`) : `"similarity": float(1 - score)  # L2 distance to similarity`.
‚Üí **Frontend** : `formatSimilarity(score) = (score * 100).toFixed(1) + "%"`.

---

### **Q11 : Pourquoi normaliser les vecteurs avec faiss.normalize_L2() ?**

‚Üí **Sans normalisation** : Distance L2 biais√©e par magnitude (vecteurs longs > courts m√™me si s√©mantiquement proches).
‚Üí **Avec normalisation** : `||a|| = ||b|| = 1` ‚Üí distance L2 √©quivalente √† **similarit√© cosinus** : `||a-b||¬≤ = 2 - 2¬∑cos(Œ∏)`.
‚Üí **Avantage** : Comparaison purement s√©mantique (angle entre vecteurs), ind√©pendante de la longueur texte.
‚Üí **Application** : `encode_query()` (ligne 204) normalise query, `IndexBuilder` normalise documents avant indexation.

---

### **Q12 : Comment fonctionne le rebuild de l'index FAISS ?**

‚Üí **Endpoint** : POST `/rebuild` avec authentification `X-API-Key` (header).
‚Üí **BackgroundTask** : Op√©ration longue (30-60s) ex√©cut√©e en async, retour imm√©diat 202 Accepted + task_id.
‚Üí **Pipeline** :
   1. `IndexBuilder.load_documents()` ‚Üí charge `rag_documents.json`
   2. Batch embedding (32 docs/batch) avec progress callbacks (0-100%)
   3. `FAISS.from_documents()` + `add_documents()` (batching)
   4. `save_local()` ‚Üí index.faiss + index.pkl + config.json
   5. `get_rag_engine.cache_clear()` ‚Üí reload index dans RAGEngine
‚Üí **S√©curit√©** : V√©rifie REBUILD_API_KEY (HTTPException 401 si invalide).

---

## ü§ñ SECTION 3 : LLM & API

### **Q13 : Quel LLM utilisez-vous et pourquoi Mistral vs OpenAI ?**

‚Üí **LLM** : `mistral-small-latest` (ChatMistralAI wrapper LangChain).
‚Üí **Avantages Mistral** :
   - Fran√ßais natif (meilleure qualit√© r√©ponses culturelles)
   - Prix : ~2x moins cher qu'OpenAI GPT-4
   - Souverainet√© europ√©enne (RGPD, donn√©es EU)
   - API unifi√©e embeddings + LLM (gestion simplifi√©e)
‚Üí **vs OpenAI GPT-4** : Plus cher, am√©ricain, mais √©cosyst√®me plus mature.
‚Üí **Alternative Mistral** : `mistral-large` (plus pr√©cis, +50% co√ªt).

---

### **Q14 : Pourquoi temp√©rature 0.7 pour g√©n√©ration et 0 pour classification ?**

‚Üí **Temp√©rature** = param√®tre randomness LLM (0 = d√©terministe, 2 = tr√®s cr√©atif).
‚Üí **Classification (temp=0)** : Sortie binaire "SEARCH"/"CHAT" doit √™tre **reproductible**, pas de cr√©ativit√© n√©cessaire.
‚Üí **G√©n√©ration (temp=0.7)** : R√©ponses conversationnelles **vari√©es** mais contr√¥l√©es (√©vite hallucinations > 1.0, √©vite r√©p√©titif < 0.5).
‚Üí **Code** : `get_llm(temperature=0.7, max_tokens=1000)` pour RAG/conversation, `get_llm(temperature=0, max_tokens=10)` pour classification.

---

### **Q15 : Comment injectez-vous le contexte RAG dans le prompt ?**

‚Üí **M√©thode** : `_format_context(results)` transforme r√©sultats FAISS en string structur√© :
```
√âv√©nements pertinents :

√âv√©nement 1 :
[page_content du Document]

√âv√©nement 2 :
...
```
‚Üí **Template RAG** : `RAG_SYSTEM_PROMPT_TEMPLATE` contient placeholder `{context}`. LangChain remplace dynamiquement.
‚Üí **Cha√Æne LCEL** : `ChatPromptTemplate([system, history, human]) | llm | parser` ‚Üí inject variables `{"context": formatted, "query": query, "history": messages}`.
‚Üí **Top-k=5** : Limite contexte √† 5 √©v√©nements (√©vite d√©passement tokens, focus pertinence).

---

### **Q16 : Quels sont les endpoints FastAPI principaux ?**

‚Üí **7 endpoints REST** :
   - `GET /health` : Health check + stats (nb docs, sessions actives)
   - `POST /search` : Recherche s√©mantique stateless (sans session)
   - `POST /chat` : Chat conversationnel avec session persistence
   - `GET /session/{id}` : R√©cup√®re historique session
   - `DELETE /session/{id}` : Supprime session + messages
   - `POST /rebuild` : Rebuild index FAISS (background, auth requise)
   - `GET /events` : Liste √©v√©nements index√©s (pagination)
‚Üí **Documentation auto** : Swagger UI √† `/docs`, ReDoc √† `/redoc`.

---

### **Q17 : Comment g√©rez-vous les sessions avec PostgreSQL ?**

‚Üí **Repository pattern** : `SessionRepository`, `MessageRepository` (async SQLAlchemy).
‚Üí **Mod√®les** :
   - `SessionModel` : id (UUID), created_at, updated_at, messages (relation)
   - `MessageModel` : id (auto), session_id (FK), role, content, sources (JSONB), latency_ms, top_k
‚Üí **Async** : `AsyncSession` (asyncpg driver) = requ√™tes DB non-bloquantes, FastAPI traite N requ√™tes parall√®les.
‚Üí **Cleanup** : Pas d'auto-expiration actuellement (am√©lioration future : TTL 24h).
‚Üí **Connection pool** : 10 connexions (`pool_size=10`), `pool_pre_ping=True` (health checks).

---

## ‚öñÔ∏è SECTION 4 : CHOIX TECHNIQUES & TRADE-OFFS

### **Q18 : Quels sont les avantages/limites du pipeline dual-mode ?**

‚Üí **‚úÖ Avantages** :
   - Performance : √âvite FAISS inutile (salutations, remerciements) ‚Üí √©conomie 50-70% requ√™tes vectorstore
   - UX : R√©ponses conversationnelles naturelles (pas de "d√©sol√©, pas d'√©v√©nement" pour "Bonjour")
   - Flexibilit√© : Gestion diff√©renci√©e prompts (CONVERSATION vs RAG)
‚Üí **‚ùå Limites** :
   - Complexit√© : 2 prompts √† maintenir (drift possible)
   - Classification peut √©chouer (~5% edge cases th√©oriques, 0% observ√© sur test)
   - Latence ajout√©e : +50-100ms pour classification (acceptable < 3s total)

---

### **Q19 : LangChain LCEL : avantages/inconv√©nients vs impl√©mentation custom ?**

‚Üí **‚úÖ Avantages** :
   - Portabilit√© : Changement provider facile (Mistral ‚Üí OpenAI = 3 lignes)
   - Abstractions : MessagesPlaceholder, Document, StrOutputParser standardis√©s
   - Communaut√© : Ecosystem LangChain (FAISS, Pinecone, agents...)
‚Üí **‚ùå Inconv√©nients** :
   - Overhead : ~10% latence vs appels API directs
   - D√©pendance : Breaking changes LangChain (migration mistral ‚Üí 0.1.x r√©cente)
   - Courbe apprentissage : LCEL syntax non intuitive d√©butants
‚Üí **Trade-off** : Gain maintenabilit√© > perte performance mineure.

---

### **Q20 : FAISS vs Pinecone : pourquoi ce choix pour votre projet ?**

‚Üí **FAISS choisi** :
   - ‚úÖ Gratuit, auto-h√©berg√© (pas de co√ªt cloud)
   - ‚úÖ Ultra-rapide pour petits datasets (<10k docs)
   - ‚úÖ Contr√¥le total donn√©es (RGPD, offline)
   - ‚ùå Scalabilit√© limit√©e RAM (~1M docs max)
   - ‚ùå Pas de distribution (single server)
‚Üí **Pinecone alternative** :
   - ‚úÖ Scalable (milliards vecteurs)
   - ‚úÖ Managed (updates, backups auto)
   - ‚ùå Co√ªt (~$70/mois minimum)
   - ‚ùå Vendor lock-in
‚Üí **D√©cision** : 497 √©v√©nements ‚Üí FAISS largement suffisant, √©vite complexit√© cloud.

---

### **Q21 : Quelles m√©triques d'√©valuation utilisez-vous (RAGAS, latence) ?**

‚Üí **3 m√©triques principales** :
   1. **Latence** : Temps total encode+search+generate. **Cible <3.0s**, **r√©sultat 2.41s** ‚úÖ (mesure `time.time()`, stock√©e `MessageModel.latency_ms`)
   2. **Couverture mots-cl√©s** : % mots attendus pr√©sents dans r√©ponse. **Cible >80%**, **r√©sultat 81.5%** ‚úÖ (annotation manuelle 12 questions)
   3. **Classification accuracy** : D√©tection SEARCH/CHAT correcte. **R√©sultat 100%** (12/12) ‚úÖ
‚Üí **RAGAS** (future) : `context_precision`, `context_recall`, `answer_relevancy`, `faithfulness` (framework evaluation RAG).
‚Üí **Dataset test** : `test-questions-annote.json` (12 questions, 5 cat√©gories).

---

### **Q22 : Quelles sont les limitations actuelles et am√©liorations futures ?**

‚Üí **Limitations** :
   - Scalabilit√© : FAISS in-memory (limite ~10k √©v√©nements)
   - Mise √† jour manuelle : Pas d'auto-sync Open Agenda (rebuild manuel via /rebuild)
   - Mono-langue : Fran√ßais uniquement (prompts, embeddings)
   - Session volatility : PostgreSQL mais pas de TTL auto (accumulation sessions)
   - Pas d'auth utilisateur : API publique (rate limiting futur)
‚Üí **Am√©liorations prioritaires** :
   - Court terme : Cache Redis sessions, authentification JWT, monitoring Prometheus
   - Moyen terme : Auto-sync Open Agenda (CRON), multi-langue (English), recommandations personnalis√©es
   - Long terme : Fine-tuning Mistral, migration Pinecone (si >10k events), interface vocale

---

## üìä ANNEXE : CHIFFRES CL√âS √Ä M√âMORISER

### Architecture
- **Pipeline** : Dual-mode (CHAT / SEARCH)
- **LCEL chains** : 3 cha√Ænes (classification, conversation, RAG)
- **Classification** : Temp√©rature 0, max_tokens=10, accuracy 100%

### Donn√©es & Index
- **√âv√©nements index√©s** : 497 (Open Agenda API)
- **Embedding dimension** : 1024d (mistral-embed)
- **Index FAISS** : IndexFlatL2, normalisation L2
- **Format LangChain** : index.faiss + index.pkl + config.json

### Performance
- **Latence moyenne** : 2.41s (cible <3.0s) ‚úÖ
- **Couverture mots-cl√©s** : 81.5% (cible >80%) ‚úÖ
- **Classification accuracy** : 100% (12/12 questions)
- **Temps rebuild index** : ~30-60s (background task)

### Stack Technique
- **Embeddings** : MistralAIEmbeddings (mistral-embed, 1024d)
- **LLM** : ChatMistralAI (mistral-small-latest, temp=0.7, max_tokens=1000)
- **Vectorstore** : FAISS IndexFlatL2 (LangChain wrapper)
- **API** : FastAPI (async/await, ASGI, 7 endpoints)
- **Database** : PostgreSQL (async SQLAlchemy, asyncpg)
- **Session management** : MAX_HISTORY=5 messages, repository pattern

### Tests & √âvaluation
- **Dataset test** : 12 questions annot√©es (5 cat√©gories)
- **M√©triques** : Latence, couverture, classification accuracy
- **Framework** : RAGAS (context_precision, answer_relevancy)
- **Coverage code** : 85% (pytest-cov)

### Configuration
- **Temp√©rature g√©n√©ration** : 0.7 (cr√©atif contr√¥l√©)
- **Temp√©rature classification** : 0 (d√©terministe)
- **Top-k retrieval** : 5 √©v√©nements max
- **MAX_HISTORY** : 5 paires (10 messages max)
- **Batch size embeddings** : 32 documents
- **Target latency** : <3.0s
- **Target relevance** : >80% coverage

---

## üéØ CONSEILS SOUTENANCE

**Questions probables examinateur** :
1. "Pourquoi RAG vs fine-tuning ?" ‚Üí Co√ªt, actualisation temps r√©el, tra√ßabilit√©
2. "Comment g√©rez-vous l'historique ?" ‚Üí PostgreSQL, MAX_HISTORY=5, MessagesPlaceholder
3. "Expliquez calcul similarit√©" ‚Üí 1 - L2_distance, normalisation vecteurs
4. "Pourquoi dual-mode ?" ‚Üí Performance (√©vite FAISS inutile), UX conversationnelle
5. "Limitations ?" ‚Üí Scalabilit√© FAISS, pas auto-sync, mono-langue

**R√©flexes techniques** :
- Toujours citer chiffres pr√©cis (2.41s, 81.5%, 1024d, 497 events)
- Justifier choix architecturaux (Mistral = fran√ßais natif + prix, FAISS = gratuit + contr√¥le)
- Montrer conscience trade-offs (LCEL overhead 10% vs portabilit√©)
- Proposer am√©liorations (cache Redis, JWT auth, multi-langue)

**Temps r√©vision** : 15-20 minutes lecture compl√®te + 5 minutes m√©morisation chiffres = pr√™t pour soutenance ! üöÄ
