# üéØ GUIDE COMPLET - Projet RAG Assistant Intelligent Events

**Date limite d√©mo : 24 janvier 2026 (14 jours)**  
**Porteur : Pierre**  
**Stack : Streamlit + LangChain + Mistral + Faiss**

---

## üìã TABLE DES MATI√àRES

1. [Vue d'ensemble du projet](#vue-densemble)
2. [Planning hebdomadaire](#planning)
3. [Checklist d√©taill√©e par √©tape](#checklist)
4. [Conseils techniques critiques](#conseils-techniques)
5. [Configuration environnement](#environnement)
6. [Structure du projet](#structure)
7. [Points de vigilance](#vigilance)
8. [Ressources et liens](#ressources)

---

## üéØ VUE D'ENSEMBLE DU PROJET {#vue-densemble}

### Objectif
Cr√©er un chatbot RAG (Retrieval-Augmented Generation) qui permet de d√©couvrir des √©v√©nements culturels via questions en langage naturel, bas√© sur les donn√©es Open Agenda.

### M√©triques de succ√®s
- ‚úÖ Pertinence r√©ponses > 80%
- ‚úÖ Temps de r√©ponse < 3 secondes
- ‚úÖ Satisfaction d√©mo > 4/5
- ‚úÖ Couverture questions > 70%

### Livrables finaux
1. ‚úÖ Application Streamlit fonctionnelle
2. ‚úÖ API REST FastAPI (optionnelle mais recommand√©e)
3. ‚úÖ Pipeline d'indexation automatis√©
4. ‚úÖ Index vectoriel Faiss
5. ‚úÖ Tests fonctionnels avec jeu de questions
6. ‚úÖ Documentation technique compl√®te
7. ‚úÖ Conteneurisation Docker
8. ‚úÖ Pr√©sentation PowerPoint (10-15 slides)

---

## üìÖ PLANNING D√âTAILL√â (14 JOURS) {#planning}

### üîµ SEMAINE 1 : D√âVELOPPEMENT CORE

#### **Jour 1-2 (10-11 janvier) : Fondations**
```
‚úÖ Configuration environnement Python
‚úÖ Installation d√©pendances (requirements.txt)
‚úÖ Test connexion API Open Agenda
‚úÖ R√©cup√©ration premi√®re donn√©e test
‚úÖ Structure projet cr√©√©e
```

**Livrables J2 :**
- Environnement virtuel fonctionnel
- `requirements.txt` complet
- Premier script de test API
- README.md initial
- Git initialis√© avec .gitignore

---

#### **Jour 3-4 (12-13 janvier) : Pipeline d'indexation**
```
‚úÖ Script r√©cup√©ration √©v√©nements Open Agenda
‚úÖ Nettoyage et structuration donn√©es
‚úÖ G√©n√©ration embeddings (test sur 50 √©v√©nements)
‚úÖ Cr√©ation index FAISS
‚úÖ Tests unitaires pipeline
```

**Livrables J4 :**
- `scripts/fetch_events.py` fonctionnel
- `scripts/build_index.py` fonctionnel
- Index FAISS avec ~100-500 √©v√©nements
- Dataset test sauvegard√© (JSON)

---

#### **Jour 5-7 (14-16 janvier) : Syst√®me RAG**
```
‚úÖ Impl√©mentation retriever (recherche similarit√©)
‚úÖ Int√©gration Mistral AI (g√©n√©ration r√©ponses)
‚úÖ Orchestration LangChain
‚úÖ Tests end-to-end RAG
‚úÖ Optimisation prompts
```

**Livrables J7 :**
- Module RAG fonctionnel
- 5 questions/r√©ponses test√©es avec succ√®s
- Temps de r√©ponse < 3s valid√©
- Logs structur√©s

---

### üü¢ SEMAINE 2 : INTERFACE & FINITIONS

#### **Jour 8-10 (17-19 janvier) : Interface Streamlit + API**
```
‚úÖ Interface Streamlit conversationnelle
‚úÖ Affichage format√© des r√©ponses
‚úÖ Liens vers √©v√©nements Open Agenda
‚úÖ API REST FastAPI (si temps)
‚úÖ Tests interface utilisateur
```

**Livrables J10 :**
- App Streamlit fonctionnelle
- Design √©pur√© et professionnel
- Gestion d'erreurs robuste
- (Optionnel) API REST avec Swagger

---

#### **Jour 11-12 (20-21 janvier) : Tests & √âvaluation**
```
‚úÖ Jeu de test 30+ questions annot√©es
‚úÖ Script d'√©valuation automatique
‚úÖ Calcul m√©triques (Recall, Precision, F1)
‚úÖ Tests sur diff√©rents sc√©narios
‚úÖ Correction bugs identifi√©s
```

**Livrables J12 :**
- `tests/test_questions.json` (30 questions)
- `scripts/evaluate_rag.py` fonctionnel
- Rapport m√©triques (CSV/JSON)
- Documentation tests

---

#### **Jour 13 (22 janvier) : Docker & Documentation**
```
‚úÖ Dockerfile optimis√©
‚úÖ Docker Compose (optionnel)
‚úÖ Tests build/run containers
‚úÖ Documentation technique compl√®te
‚úÖ README final
```

**Livrables J13 :**
- `Dockerfile` fonctionnel
- `docker-compose.yml` (optionnel)
- Documentation installation
- Guide troubleshooting

---

#### **Jour 14 (23 janvier) : Pr√©sentation & R√©p√©tition**
```
‚úÖ PowerPoint finalis√© (10-15 slides)
‚úÖ Sc√©narios d√©mo pr√©par√©s
‚úÖ Vid√©o backup enregistr√©e
‚úÖ R√©p√©tition pr√©sentation
‚úÖ Tests finaux
```

**Livrables J14 :**
- PowerPoint complet
- Script d√©mo avec exemples
- Vid√©o backup (au cas o√π)
- Syst√®me 100% op√©rationnel

---

#### **üé¨ Jour 15 (24 janvier) : D√âMO**
```
‚úÖ Pr√©sentation 20 minutes
‚úÖ D√©mo live interactive
‚úÖ Q&A
‚úÖ Feedback √©quipes
```

---

## ‚úÖ CHECKLIST D√âTAILL√âE PAR √âTAPE {#checklist}

### üì¶ √âTAPE 1 : Configuration Environnement

#### Actions
```bash
# 1. Cr√©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 2. Cr√©er requirements.txt
touch requirements.txt

# 3. Installer d√©pendances
pip install -r requirements.txt

# 4. Tester imports critiques
python -c "import faiss; print('Faiss OK')"
python -c "from langchain_community.vectorstores import FAISS; print('LangChain OK')"
python -c "from mistralai import Mistral; print('Mistral OK')"
```

#### ‚úÖ Checklist
- [ ] Environnement virtuel cr√©√© et activ√©
- [ ] `requirements.txt` complet (voir section Environnement)
- [ ] Toutes d√©pendances install√©es sans erreur
- [ ] Imports critiques test√©s
- [ ] `.env` cr√©√© avec cl√©s API (non versionn√©)
- [ ] `.gitignore` configur√©
- [ ] Git initialis√©

#### Validation
```bash
# Tous ces imports doivent r√©ussir
python << EOF
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from mistralai import Mistral
import streamlit
import pandas
import requests
print("‚úÖ Environnement OK")
EOF
```

---

### üì° √âTAPE 2 : R√©cup√©ration Donn√©es Open Agenda

#### Actions
```python
# scripts/fetch_events.py
import requests
import json
from datetime import datetime, timedelta

def fetch_open_agenda_events(location="paris", max_events=500):
    """
    R√©cup√®re √©v√©nements depuis Open Agenda
    
    Args:
        location: ville/r√©gion (ex: "paris", "lyon", "marseille")
        max_events: nombre maximum d'√©v√©nements
    
    Returns:
        list: √©v√©nements structur√©s
    """
    base_url = "https://api.openagenda.com/v2/agendas/{agenda_uid}/events"
    
    # TODO: Obtenir votre agenda_uid
    # Documentation: https://openagenda.zendesk.com/hc/fr
    
    events = []
    # Impl√©menter pagination API
    # Filtrer par date (derniers 12 mois + futurs)
    # Nettoyer HTML descriptions
    
    return events

if __name__ == "__main__":
    events = fetch_open_agenda_events("paris", 1000)
    print(f"‚úÖ {len(events)} √©v√©nements r√©cup√©r√©s")
    
    # Sauvegarder pour tests
    with open("data/events_raw.json", "w") as f:
        json.dump(events, f, ensure_ascii=False, indent=2)
```

#### ‚úÖ Checklist
- [ ] Compte Open Agenda cr√©√©
- [ ] API key r√©cup√©r√©e (si n√©cessaire)
- [ ] Documentation API lue
- [ ] Script `fetch_events.py` cr√©√©
- [ ] Test r√©cup√©ration 10 √©v√©nements r√©ussi
- [ ] R√©cup√©ration compl√®te (500-2000 √©v√©nements)
- [ ] Donn√©es sauvegard√©es en JSON
- [ ] Nettoyage HTML effectu√©
- [ ] Validation structure donn√©es

#### Structure attendue (JSON)
```json
{
  "id": "12345",
  "title": "Concert de Jazz au Caveau",
  "description": "Soir√©e jazz manouche...",
  "location": {
    "city": "Paris",
    "address": "15 rue de la Huchette",
    "coordinates": {"lat": 48.85, "lon": 2.35}
  },
  "dates": {
    "start": "2026-01-20T20:00:00",
    "end": "2026-01-20T23:00:00"
  },
  "price": "15‚Ç¨",
  "category": "Concert",
  "url": "https://openagenda.com/event/12345"
}
```

#### Points de vigilance
- ‚ö†Ô∏è Gestion pagination API
- ‚ö†Ô∏è Rate limiting (respecter limites)
- ‚ö†Ô∏è Nettoyage HTML (BeautifulSoup ou regex)
- ‚ö†Ô∏è Validation donn√©es (√©viter champs vides)
- ‚ö†Ô∏è Encodage UTF-8

---

### üîç √âTAPE 3 : Indexation Vectorielle FAISS

#### Actions
```python
# scripts/build_index.py
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document
import json

def build_faiss_index(events_file="data/events_raw.json"):
    """
    Construit index FAISS depuis √©v√©nements
    
    Args:
        events_file: chemin fichier JSON √©v√©nements
    
    Returns:
        FAISS: index vectoriel
    """
    # 1. Charger √©v√©nements
    with open(events_file) as f:
        events = json.load(f)
    
    # 2. Cr√©er embeddings model
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # 3. Cr√©er documents LangChain
    documents = []
    for event in events:
        # Combiner informations pour recherche s√©mantique
        content = f"""
        Titre: {event['title']}
        Description: {event['description']}
        Lieu: {event['location']['city']}
        Date: {event['dates']['start']}
        Prix: {event.get('price', 'Non sp√©cifi√©')}
        Cat√©gorie: {event.get('category', '√âv√©nement')}
        """
        
        doc = Document(
            page_content=content,
            metadata={
                "title": event['title'],
                "city": event['location']['city'],
                "date": event['dates']['start'],
                "price": event.get('price'),
                "url": event['url'],
                "id": event['id']
            }
        )
        documents.append(doc)
    
    # 4. Cr√©er index FAISS
    print(f"üî® Indexation de {len(documents)} √©v√©nements...")
    vectorstore = FAISS.from_documents(documents, embeddings)
    
    # 5. Sauvegarder
    vectorstore.save_local("data/faiss_index")
    print("‚úÖ Index sauvegard√© dans data/faiss_index")
    
    return vectorstore

if __name__ == "__main__":
    index = build_faiss_index()
    
    # Test recherche
    results = index.similarity_search("concert jazz paris", k=3)
    for i, doc in enumerate(results, 1):
        print(f"\n{i}. {doc.metadata['title']}")
```

#### ‚úÖ Checklist
- [ ] Script `build_index.py` cr√©√©
- [ ] Mod√®le embeddings t√©l√©charg√©
- [ ] Documents LangChain cr√©√©s correctement
- [ ] Index FAISS construit sans erreur
- [ ] Index sauvegard√© sur disque
- [ ] Test recherche similarit√© r√©ussi
- [ ] Temps indexation acceptable (< 5 min pour 1000 √©v√©nements)

#### Validation
```python
# Test complet
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Charger index
vectorstore = FAISS.load_local(
    "data/faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# Test recherche
queries = [
    "concert jazz ce weekend",
    "exposition gratuite paris",
    "spectacle enfants dimanche"
]

for query in queries:
    results = vectorstore.similarity_search(query, k=3)
    print(f"\nüîç Query: {query}")
    for doc in results:
        print(f"  - {doc.metadata['title']}")
```

#### Points de vigilance
- ‚ö†Ô∏è Taille mod√®le embeddings (choisir l√©ger pour POC)
- ‚ö†Ô∏è M√©moire RAM (indexation peut consommer beaucoup)
- ‚ö†Ô∏è Format m√©tadonn√©es (doit √™tre JSON serializable)
- ‚ö†Ô∏è Persistence index (save_local/load_local)

---

### ü§ñ √âTAPE 4 : Syst√®me RAG (Retrieval + Generation)

#### Actions
```python
# rag/chatbot.py
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_mistralai import ChatMistralAI
from langchain.prompts import PromptTemplate
import os

class EventChatbot:
    def __init__(self, index_path="data/faiss_index"):
        # 1. Charger index FAISS
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        self.vectorstore = FAISS.load_local(
            index_path,
            embeddings,
            allow_dangerous_deserialization=True
        )
        
        # 2. Initialiser Mistral
        self.llm = ChatMistralAI(
            model="mistral-small-latest",
            mistral_api_key=os.getenv("MISTRAL_API_KEY"),
            temperature=0.3
        )
        
        # 3. D√©finir prompt
        template = """Tu es un assistant intelligent qui aide les utilisateurs √† d√©couvrir des √©v√©nements culturels.
        
Contexte des √©v√©nements trouv√©s :
{context}

Question de l'utilisateur : {question}

Instructions :
- R√©ponds en fran√ßais de mani√®re naturelle et conversationnelle
- Recommande 2-3 √©v√©nements pertinents maximum
- Mentionne les informations pratiques (date, lieu, prix)
- Si aucun √©v√©nement ne correspond, propose des alternatives
- Sois concis mais informatif

R√©ponse :"""
        
        self.prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        # 4. Cr√©er cha√Æne RAG
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}
            ),
            chain_type_kwargs={"prompt": self.prompt},
            return_source_documents=True
        )
    
    def ask(self, question):
        """
        Pose une question au chatbot
        
        Args:
            question: question utilisateur
        
        Returns:
            dict: {
                "answer": r√©ponse g√©n√©r√©e,
                "sources": √©v√©nements sources avec m√©tadonn√©es
            }
        """
        result = self.qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "sources": [
                {
                    "title": doc.metadata["title"],
                    "url": doc.metadata["url"],
                    "date": doc.metadata["date"],
                    "city": doc.metadata["city"],
                    "price": doc.metadata.get("price", "Non sp√©cifi√©")
                }
                for doc in result["source_documents"]
            ]
        }

if __name__ == "__main__":
    # Test
    chatbot = EventChatbot()
    
    test_questions = [
        "Quels concerts ce weekend √† Paris ?",
        "√âv√©nements gratuits pour enfants dimanche ?",
        "Exposition d'art contemporain dans le Marais ?"
    ]
    
    for q in test_questions:
        print(f"\nüîµ Q: {q}")
        result = chatbot.ask(q)
        print(f"ü§ñ A: {result['answer']}")
        print(f"üìö Sources: {len(result['sources'])} √©v√©nements")
```

#### ‚úÖ Checklist
- [ ] Cl√© API Mistral configur√©e
- [ ] Module RAG cr√©√© (`rag/chatbot.py`)
- [ ] Retriever fonctionnel (top-k √©v√©nements)
- [ ] LLM Mistral int√©gr√©
- [ ] Prompt optimis√© (it√©rations)
- [ ] Tests avec 10 questions vari√©es
- [ ] Temps de r√©ponse < 3s
- [ ] Pertinence r√©ponses valid√©e

#### Validation
```python
# Test end-to-end
from rag.chatbot import EventChatbot
import time

chatbot = EventChatbot()

questions = [
    "Concert jazz pas cher ce weekend",
    "Activit√©s en famille samedi",
    "Exposition photo gratuite"
]

for q in questions:
    start = time.time()
    result = chatbot.ask(q)
    latency = time.time() - start
    
    print(f"\n{'='*60}")
    print(f"Q: {q}")
    print(f"A: {result['answer'][:200]}...")
    print(f"‚è±Ô∏è Latency: {latency:.2f}s")
    print(f"üìä Sources: {len(result['sources'])}")
    
    assert latency < 3.0, f"‚ùå Latence trop √©lev√©e: {latency}s"
    assert len(result['sources']) > 0, "‚ùå Pas de sources"
    
print("\n‚úÖ Tous les tests passent")
```

#### Points de vigilance
- ‚ö†Ô∏è Co√ªt tokens Mistral (limiter contexte)
- ‚ö†Ô∏è Qualit√© prompt (it√©rer jusqu'√† satisfaction)
- ‚ö†Ô∏è Gestion erreurs API
- ‚ö†Ô∏è Temp√©rature LLM (0.3 pour coh√©rence)
- ‚ö†Ô∏è Nombre de documents r√©cup√©r√©s (k=5 recommand√©)

---

### üé® √âTAPE 5 : Interface Streamlit

#### Actions
```python
# app.py
import streamlit as st
from rag.chatbot import EventChatbot
import time

# Configuration page
st.set_page_config(
    page_title="Assistant √âv√©nements",
    page_icon="üé≠",
    layout="wide"
)

# Titre
st.title("üé≠ Assistant Intelligent √âv√©nements")
st.markdown("D√©couvrez des √©v√©nements culturels en posant vos questions naturellement")

# Initialiser chatbot (cache)
@st.cache_resource
def load_chatbot():
    return EventChatbot()

chatbot = load_chatbot()

# Zone de saisie
question = st.text_input(
    "Posez votre question :",
    placeholder="Ex: Concerts jazz ce weekend √† Paris pas cher"
)

if st.button("üîç Rechercher") and question:
    with st.spinner("Recherche en cours..."):
        start = time.time()
        result = chatbot.ask(question)
        latency = time.time() - start
    
    # Afficher r√©ponse
    st.markdown("### üí¨ R√©ponse")
    st.write(result["answer"])
    
    # Afficher sources
    st.markdown("### üìö √âv√©nements sugg√©r√©s")
    for i, source in enumerate(result["sources"], 1):
        with st.expander(f"{i}. {source['title']}"):
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"üìÖ **Date:** {source['date']}")
                st.write(f"üìç **Lieu:** {source['city']}")
            with col2:
                st.write(f"üí∞ **Prix:** {source['price']}")
                st.write(f"üîó [Voir sur Open Agenda]({source['url']})")
    
    # M√©triques (optionnel)
    st.sidebar.metric("‚è±Ô∏è Temps de r√©ponse", f"{latency:.2f}s")
    st.sidebar.metric("üìä Sources", len(result["sources"]))

# Exemples (sidebar)
st.sidebar.markdown("### üí° Exemples de questions")
st.sidebar.markdown("""
- Concerts ce weekend √† Paris ?
- √âv√©nements gratuits pour enfants ?
- Exposition d'art dans le Marais ?
- Spectacles samedi soir pas cher ?
""")
```

#### ‚úÖ Checklist
- [ ] `app.py` cr√©√©
- [ ] Interface conversationnelle fonctionnelle
- [ ] Affichage r√©ponse format√©e
- [ ] Sources avec liens cliquables
- [ ] Design √©pur√© et professionnel
- [ ] Gestion erreurs (question vide, API down)
- [ ] Indicateurs de chargement
- [ ] Tests sur diff√©rents navigateurs

#### Lancement
```bash
streamlit run app.py
```

#### Points de vigilance
- ‚ö†Ô∏è Cache chatbot (@st.cache_resource)
- ‚ö†Ô∏è UX/UI simple et claire
- ‚ö†Ô∏è Messages d'erreur compr√©hensibles
- ‚ö†Ô∏è Responsive design (desktop priority)

---

### üåê √âTAPE 6 : API REST FastAPI (OPTIONNEL)

#### Actions
```python
# api/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from rag.chatbot import EventChatbot
import time

app = FastAPI(
    title="API Assistant √âv√©nements",
    description="API REST pour syst√®me RAG √©v√©nements culturels",
    version="1.0.0"
)

# Charger chatbot au d√©marrage
chatbot = None

@app.on_event("startup")
async def startup_event():
    global chatbot
    chatbot = EventChatbot()

class Question(BaseModel):
    query: str
    max_results: int = 5

class Answer(BaseModel):
    answer: str
    sources: list
    latency: float

@app.post("/ask", response_model=Answer)
async def ask_question(question: Question):
    """
    Pose une question au syst√®me RAG
    
    Args:
        question: objet Question avec query
    
    Returns:
        Answer avec r√©ponse, sources et latence
    """
    if not question.query.strip():
        raise HTTPException(status_code=400, detail="Question vide")
    
    try:
        start = time.time()
        result = chatbot.ask(question.query)
        latency = time.time() - start
        
        return Answer(
            answer=result["answer"],
            sources=result["sources"][:question.max_results],
            latency=latency
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "model": "mistral-small"}

@app.post("/rebuild")
async def rebuild_index():
    """Reconstruit l'index vectoriel (admin)"""
    # TODO: Impl√©menter si n√©cessaire
    return {"message": "Not implemented in POC"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### ‚úÖ Checklist (si API impl√©ment√©e)
- [ ] `api/main.py` cr√©√©
- [ ] Endpoints `/ask`, `/health` fonctionnels
- [ ] Swagger auto-g√©n√©r√© accessible (/docs)
- [ ] Tests avec curl/Postman
- [ ] Gestion erreurs robuste
- [ ] Documentation OpenAPI compl√®te

#### Test
```bash
# Lancer API
uvicorn api.main:app --reload

# Tester
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"query": "concerts jazz paris"}'

# Swagger
open http://localhost:8000/docs
```

---

### ‚úÖ √âTAPE 7 : Tests & √âvaluation

#### Actions
```python
# tests/test_questions.json
[
  {
    "id": 1,
    "question": "Quels concerts jazz ce weekend √† Paris ?",
    "expected_keywords": ["concert", "jazz", "paris", "weekend"],
    "category": "recherche_simple"
  },
  {
    "id": 2,
    "question": "√âv√©nements gratuits pour enfants dimanche apr√®s-midi",
    "expected_keywords": ["gratuit", "enfants", "dimanche"],
    "category": "filtres_multiples"
  }
  // ... 30 questions total
]
```

```python
# scripts/evaluate_rag.py
import json
from rag.chatbot import EventChatbot
import time

def evaluate_rag(test_file="tests/test_questions.json"):
    """√âvalue le syst√®me RAG sur jeu de test"""
    
    chatbot = EventChatbot()
    
    with open(test_file) as f:
        test_cases = json.load(f)
    
    results = []
    total_latency = 0
    
    for test in test_cases:
        print(f"\nüîç Test {test['id']}: {test['question']}")
        
        start = time.time()
        result = chatbot.ask(test['question'])
        latency = time.time() - start
        total_latency += latency
        
        # V√©rifier pr√©sence mots-cl√©s
        answer_lower = result['answer'].lower()
        keywords_found = sum(
            1 for kw in test['expected_keywords']
            if kw.lower() in answer_lower
        )
        
        relevance = keywords_found / len(test['expected_keywords'])
        
        results.append({
            "id": test['id'],
            "question": test['question'],
            "answer": result['answer'],
            "latency": latency,
            "relevance_score": relevance,
            "sources_count": len(result['sources'])
        })
        
        print(f"  ‚è±Ô∏è Latency: {latency:.2f}s")
        print(f"  üìä Relevance: {relevance:.0%}")
        print(f"  üìö Sources: {len(result['sources'])}")
    
    # M√©triques globales
    avg_latency = total_latency / len(test_cases)
    avg_relevance = sum(r['relevance_score'] for r in results) / len(results)
    
    print(f"\n{'='*60}")
    print(f"üìä R√âSULTATS GLOBAUX")
    print(f"{'='*60}")
    print(f"Questions test√©es: {len(test_cases)}")
    print(f"Latence moyenne: {avg_latency:.2f}s")
    print(f"Pertinence moyenne: {avg_relevance:.0%}")
    print(f"Cible latence: < 3s ({'‚úÖ' if avg_latency < 3 else '‚ùå'})")
    print(f"Cible pertinence: > 80% ({'‚úÖ' if avg_relevance > 0.8 else '‚ùå'})")
    
    # Sauvegarder r√©sultats
    with open("tests/evaluation_results.json", "w") as f:
        json.dump({
            "results": results,
            "metrics": {
                "avg_latency": avg_latency,
                "avg_relevance": avg_relevance,
                "total_tests": len(test_cases)
            }
        }, f, indent=2, ensure_ascii=False)
    
    return results

if __name__ == "__main__":
    evaluate_rag()
```

#### ‚úÖ Checklist
- [ ] Jeu de test cr√©√© (30+ questions)
- [ ] Questions couvrent diff√©rents sc√©narios
- [ ] Script d'√©valuation fonctionnel
- [ ] M√©triques calcul√©es automatiquement
- [ ] R√©sultats sauvegard√©s (JSON/CSV)
- [ ] Cibles atteintes (latence < 3s, pertinence > 80%)

---

### üê≥ √âTAPE 8 : Conteneurisation Docker

#### Actions
```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# D√©pendances syst√®me
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# D√©pendances Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier code
COPY . .

# T√©l√©charger mod√®le embeddings (cache)
RUN python -c "from sentence_transformers import SentenceTransformer; \
    SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"

# Port Streamlit
EXPOSE 8501

# Commande d√©marrage
CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

```yaml
# docker-compose.yml (optionnel)
version: '3.8'

services:
  streamlit:
    build: .
    ports:
      - "8501:8501"
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
    volumes:
      - ./data:/app/data
    restart: unless-stopped
  
  api:  # Si API impl√©ment√©e
    build: .
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
    volumes:
      - ./data:/app/data
    restart: unless-stopped
```

#### ‚úÖ Checklist
- [ ] `Dockerfile` cr√©√© et optimis√©
- [ ] `docker-compose.yml` cr√©√© (optionnel)
- [ ] `.dockerignore` configur√©
- [ ] Build image r√©ussit (< 2GB)
- [ ] Container d√©marre sans erreur
- [ ] Application accessible (localhost:8501)
- [ ] Variables d'environnement fonctionnelles
- [ ] Tests dans container

#### Commandes
```bash
# Build
docker build -t rag-events .

# Run
docker run -p 8501:8501 \
  -e MISTRAL_API_KEY=$MISTRAL_API_KEY \
  -v $(pwd)/data:/app/data \
  rag-events

# Avec Docker Compose
docker-compose up -d

# V√©rifier logs
docker-compose logs -f
```

---

### üìä √âTAPE 9 : Pr√©sentation PowerPoint

#### Structure (10-15 slides)

**Slide 1: Titre**
- Titre: "Assistant Intelligent √âv√©nements - POC RAG"
- Nom, Date, Logo √©cole

**Slide 2: Contexte & Probl√©matique**
- Difficult√© actuelle: interfaces complexes, filtres multiples
- Besoin: recherche naturelle en langage courant
- Opportunit√©: IA conversationnelle

**Slide 3: Solution Propos√©e**
- Chatbot RAG bas√© sur Open Agenda
- Comprend langage naturel
- R√©pond avec √©v√©nements pertinents

**Slide 4: Architecture Technique**
- Sch√©ma: Utilisateur ‚Üí Streamlit ‚Üí RAG ‚Üí FAISS + Mistral
- Stack: Python, LangChain, Faiss, Mistral

**Slide 5: Pipeline de Donn√©es**
- R√©cup√©ration Open Agenda
- Nettoyage et structuration
- Vectorisation (embeddings)
- Indexation FAISS

**Slide 6-7: D√©monstration Interface**
- Screenshots Streamlit
- Exemples questions/r√©ponses
- Mise en avant UX

**Slide 8: R√©sultats & M√©triques**
- Latence moyenne: X.Xs (< 3s ‚úÖ)
- Pertinence: XX% (> 80% ‚úÖ)
- XX √©v√©nements index√©s
- XX questions test√©es

**Slide 9: Exemple Conversations**
- 2-3 captures d'√©cran conversations r√©elles
- Montrer vari√©t√© cas d'usage

**Slide 10: Limitations POC**
- Zone g√©ographique limit√©e
- Pas d'historique persistant
- Mise √† jour manuelle index
- Co√ªt tokens

**Slide 11: Pistes d'Am√©lioration**
- **Court terme**: filtres avanc√©s, plus de villes
- **Moyen terme**: personnalisation, historique
- **Long terme**: multimodal, r√©servation

**Slide 12: Recommandations**
- Go/No-go pour V1 production
- Ressources n√©cessaires
- Roadmap 6 mois

**Slide 13: Conclusion**
- POC valide la faisabilit√©
- Architecture scalable d√©montr√©e
- Pr√™t pour phase suivante

**Slide 14: Q&A**
- Questions ?
- Contact

#### ‚úÖ Checklist
- [ ] PowerPoint cr√©√© (10-15 slides)
- [ ] Design professionnel et coh√©rent
- [ ] Screenshots application inclus
- [ ] M√©triques et graphiques clairs
- [ ] Sch√©mas architecture
- [ ] Pas de jargon excessif
- [ ] Lisible et √©pur√©
- [ ] R√©p√©tition pr√©sentation (< 20 min)

---

## ‚öôÔ∏è CONFIGURATION ENVIRONNEMENT {#environnement}

### requirements.txt COMPLET

```txt
# === CORE RAG ===
langchain==0.1.0
langchain-community==0.1.0
langchain-mistralai==0.0.5
mistralai==0.1.6

# === VECTOR STORE ===
faiss-cpu==1.7.4
sentence-transformers==2.3.1

# === INTERFACE ===
streamlit==1.29.0

# === API (Optionnel) ===
fastapi==0.109.0
uvicorn[standard]==0.25.0
pydantic==2.5.3

# === DATA PROCESSING ===
requests==2.31.0
python-dotenv==1.0.0
pandas==2.1.4
beautifulsoup4==4.12.2

# === TESTS & EVALUATION ===
pytest==7.4.3
ragas==0.1.0  # Optionnel pour √©valuation avanc√©e

# === UTILS ===
python-json-logger==2.0.7
```

### Fichier .env

```bash
# .env
MISTRAL_API_KEY=your_mistral_api_key_here
OPENAGENDA_API_KEY=your_openagenda_key_if_needed

# Optionnel
LOG_LEVEL=INFO
INDEX_PATH=data/faiss_index
```

### .gitignore

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/

# Data
data/
*.json
*.csv
!tests/test_questions.json

# Secrets
.env
*.key

# OS
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/
*.swp

# Docker
.dockerignore
```

---

## üóÇÔ∏è STRUCTURE PROJET {#structure}

```
rag-events-poc/
‚îÇ
‚îú‚îÄ‚îÄ README.md                   # Documentation principale
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python
‚îú‚îÄ‚îÄ .env                        # Variables d'environnement (non versionn√©)
‚îú‚îÄ‚îÄ .gitignore                  # Fichiers ignor√©s Git
‚îú‚îÄ‚îÄ Dockerfile                  # Conteneurisation
‚îú‚îÄ‚îÄ docker-compose.yml          # Orchestration (optionnel)
‚îÇ
‚îú‚îÄ‚îÄ app.py                      # Application Streamlit
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ fetch_events.py         # R√©cup√©ration Open Agenda
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py          # Construction index FAISS
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_rag.py         # √âvaluation automatique
‚îÇ
‚îú‚îÄ‚îÄ rag/                        # Module RAG
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py              # Classe principale chatbot
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py            # Logique retrieval (optionnel)
‚îÇ   ‚îî‚îÄ‚îÄ generator.py            # Logique generation (optionnel)
‚îÇ
‚îú‚îÄ‚îÄ api/                        # API REST (optionnel)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # FastAPI app
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Donn√©es (non versionn√©)
‚îÇ   ‚îú‚îÄ‚îÄ events_raw.json         # √âv√©nements bruts
‚îÇ   ‚îú‚îÄ‚îÄ faiss_index/            # Index vectoriel
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # Tests
‚îÇ   ‚îú‚îÄ‚îÄ test_questions.json     # Jeu de test
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results.json # R√©sultats √©valuation
‚îÇ   ‚îî‚îÄ‚îÄ test_rag.py             # Tests unitaires
‚îÇ
‚îî‚îÄ‚îÄ docs/                       # Documentation
    ‚îú‚îÄ‚îÄ presentation.pptx       # PowerPoint d√©mo
    ‚îú‚îÄ‚îÄ rapport_technique.md    # Rapport technique
    ‚îî‚îÄ‚îÄ architecture.png        # Sch√©mas
```

---

## ‚ö†Ô∏è POINTS DE VIGILANCE CRITIQUES {#vigilance}

### üî¥ Erreurs fr√©quentes √† √©viter

#### 1. Imports LangChain (CRITIQUE)
```python
# ‚ùå FAUX - imports deprecated
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ‚úÖ CORRECT - imports actuels
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
```

#### 2. Mistral API
```python
# ‚ùå FAUX
from mistral import MistralClient

# ‚úÖ CORRECT
from mistralai import Mistral
# OU
from langchain_mistralai import ChatMistralAI  # Recommand√© avec LangChain
```

#### 3. Cl√© API Mistral
- ‚ö†Ô∏è NE JAMAIS versionner dans Git
- ‚úÖ Utiliser variable d'environnement
- ‚úÖ Fichier `.env` avec `python-dotenv`

#### 4. Faiss deserialization
```python
# ‚úÖ IMPORTANT: allow_dangerous_deserialization=True requis
vectorstore = FAISS.load_local(
    "data/faiss_index",
    embeddings,
    allow_dangerous_deserialization=True  # REQUIS
)
```

#### 5. Rate Limiting API
- ‚ö†Ô∏è Open Agenda: respecter limites
- ‚ö†Ô∏è Mistral: surveiller quota tokens
- ‚úÖ Impl√©menter retry logic
- ‚úÖ Cacher r√©sultats quand possible

#### 6. M√©moire RAM
- ‚ö†Ô∏è Indexation 1000+ √©v√©nements = ~2GB RAM
- ‚úÖ Surveiller consommation
- ‚úÖ Batch processing si n√©cessaire

#### 7. Docker
- ‚ö†Ô∏è Image peut √™tre lourde (1-2GB)
- ‚úÖ .dockerignore pour exclure data/
- ‚úÖ Multi-stage build si possible
- ‚úÖ Variables d'environnement pour secrets

---

## üéØ CONSEILS TECHNIQUES AVANC√âS {#conseils-techniques}

### Optimisation Performance

#### 1. Cache Streamlit
```python
@st.cache_resource
def load_chatbot():
    """Cache chatbot pour √©viter rechargement"""
    return EventChatbot()
```

#### 2. Lazy Loading
```python
# Charger index seulement quand n√©cessaire
class EventChatbot:
    def __init__(self):
        self._vectorstore = None
    
    @property
    def vectorstore(self):
        if self._vectorstore is None:
            self._vectorstore = self._load_index()
        return self._vectorstore
```

#### 3. R√©duire Taille Contexte
```python
# Limiter longueur descriptions dans embeddings
def truncate_text(text, max_words=100):
    words = text.split()
    return ' '.join(words[:max_words])
```

### Am√©lioration Qualit√©

#### 1. Prompt Engineering
```python
# Tester plusieurs formulations
prompts = [
    "Tu es un assistant...",
    "En tant qu'expert √©v√©nements...",
    "Aide l'utilisateur √† trouver..."
]

# A/B test et mesurer pertinence
```

#### 2. Hybrid Search (optionnel)
```python
# Combiner recherche s√©mantique + filtres m√©tadonn√©es
def hybrid_search(query, filters=None):
    # 1. Recherche s√©mantique
    semantic_results = vectorstore.similarity_search(query, k=20)
    
    # 2. Filtrer par m√©tadonn√©es
    if filters:
        filtered = [
            doc for doc in semantic_results
            if matches_filters(doc.metadata, filters)
        ]
        return filtered[:5]
    
    return semantic_results[:5]
```

#### 3. Reranking (optionnel)
```python
# Re-classer r√©sultats par pertinence
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_results(query, documents):
    pairs = [[query, doc.page_content] for doc in documents]
    scores = reranker.predict(pairs)
    
    # Trier par score
    ranked = sorted(
        zip(documents, scores),
        key=lambda x: x[1],
        reverse=True
    )
    return [doc for doc, score in ranked]
```

### Monitoring

#### 1. Logs Structur√©s
```python
import logging
import json_logging

json_logging.init_fastapi(enable_json=True)
logger = logging.getLogger(__name__)

def ask(self, question):
    logger.info("query_received", extra={"query": question})
    
    start = time.time()
    result = self.qa_chain({"query": question})
    latency = time.time() - start
    
    logger.info("query_completed", extra={
        "query": question,
        "latency": latency,
        "sources_count": len(result["source_documents"])
    })
    
    return result
```

#### 2. M√©triques Temps R√©el
```python
# Streamlit sidebar
st.sidebar.metric("Questions totales", session_state.total_queries)
st.sidebar.metric("Latence moyenne", f"{session_state.avg_latency:.2f}s")
st.sidebar.metric("Taux succ√®s", f"{session_state.success_rate:.0%}")
```

---

## üìö RESSOURCES & LIENS {#ressources}

### Documentation Officielle

- **Open Agenda API**: https://openagenda.zendesk.com/hc/fr
- **Mistral AI**: https://docs.mistral.ai/
- **LangChain**: https://python.langchain.com/docs/get_started
- **Faiss**: https://github.com/facebookresearch/faiss/wiki
- **Streamlit**: https://docs.streamlit.io/
- **FastAPI**: https://fastapi.tiangolo.com/

### Tutoriels Recommand√©s

- LangChain RAG: https://python.langchain.com/docs/use_cases/question_answering/
- Mistral + LangChain: https://python.langchain.com/docs/integrations/chat/mistralai/
- Faiss Best Practices: https://github.com/facebookresearch/faiss/wiki/Faiss-building-blocks

### Mod√®les Embeddings

- **all-MiniLM-L6-v2** (recommand√© POC): 384 dimensions, rapide
- **all-mpnet-base-v2**: 768 dimensions, plus pr√©cis mais plus lent
- **paraphrase-multilingual-MiniLM-L12-v2**: multilingue

### Outils Utiles

- **Postman**: tester API REST
- **DB Browser**: visualiser donn√©es
- **Weights & Biases**: tracking exp√©riences (optionnel)

---

## üöÄ COMMANDES RAPIDES

### Setup Initial
```bash
# Cr√©er environnement
python -m venv venv
source venv/bin/activate

# Installer d√©pendances
pip install -r requirements.txt

# Configurer secrets
cp .env.example .env
# √âditer .env avec vos cl√©s
```

### D√©veloppement
```bash
# Fetch data
python scripts/fetch_events.py

# Build index
python scripts/build_index.py

# Run Streamlit
streamlit run app.py

# Run API (si impl√©ment√©e)
uvicorn api.main:app --reload
```

### Tests
```bash
# Tests unitaires
pytest tests/

# √âvaluation RAG
python scripts/evaluate_rag.py
```

### Docker
```bash
# Build & Run
docker-compose up -d

# Logs
docker-compose logs -f

# Stop
docker-compose down
```

---

## ‚úÖ CHECKLIST FINALE AVANT D√âMO

### 48h Avant (22 janvier)
- [ ] Tous les scripts fonctionnent sans erreur
- [ ] Index vectoriel complet et test√©
- [ ] Application Streamlit professionnelle
- [ ] 30+ questions test√©es avec succ√®s
- [ ] M√©triques valid√©es (latence < 3s, pertinence > 80%)
- [ ] Docker build et run OK
- [ ] PowerPoint finalis√©
- [ ] Documentation compl√®te

### 24h Avant (23 janvier)
- [ ] R√©p√©tition pr√©sentation (timing)
- [ ] Sc√©narios d√©mo pr√©par√©s (3-5 exemples)
- [ ] Vid√©o backup enregistr√©e
- [ ] Environnement d√©mo test√© (laptop/connexion)
- [ ] Plan B si probl√®me technique
- [ ] Slides relues (typos, clart√©)
- [ ] R√©ponses aux questions attendues pr√©par√©es

### Jour J (24 janvier)
- [ ] ‚òï Bien repos√©
- [ ] Laptop charg√© + chargeur
- [ ] Connexion Internet stable
- [ ] Docker containers pr√©-d√©marr√©s
- [ ] Slides ouverts
- [ ] Terminal pr√™t
- [ ] Enthousiasme et confiance !

---

## üéì CONCLUSION

Ce projet RAG est ambitieux mais r√©alisable en 14 jours avec une approche m√©thodique :

**Cl√©s du succ√®s :**
1. **Prioriser impitoyablement** : focus sur fonctionnalit√©s core
2. **Tester t√¥t et souvent** : validation continue
3. **Documenter au fur et √† mesure** : pas tout √† la fin
4. **Pr√©parer la d√©mo d√®s J10** : ne pas attendre derni√®re minute
5. **Rester simple** : MVP > complexit√© pr√©matur√©e

**Si manque de temps :**
- ‚ùå Sacrifier : API REST, tests unitaires exhaustifs, nice-to-have
- ‚úÖ Garder : Streamlit, RAG core, d√©mo fluide, pr√©sentation solide

**Bon courage Pierre ! üöÄ**

N'oubliez pas : un POC bien pr√©sent√© vaut mieux qu'un syst√®me parfait mal d√©montr√©.

---

**Version** : 1.0  
**Derni√®re mise √† jour** : 10 janvier 2026  
**Auteur** : Claude (Assistant AI)
